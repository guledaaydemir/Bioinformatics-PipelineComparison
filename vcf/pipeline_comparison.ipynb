{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e25276",
   "metadata": {},
   "source": [
    "# Bioinformatic - Pipeline Comparison for BLG 604E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18c4e6",
   "metadata": {},
   "source": [
    "## OUTLINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce86abe",
   "metadata": {},
   "source": [
    "#### PART A) RESULTS FROM THE PIPELINES\n",
    "1- Reading SNP vcf filenames generated as a result of the pipeline.\n",
    "<br> 2- Creating a set for each dataframe with using \"CHROM\",\"POS\",\"REF\",\"ALT_1\" columns.\n",
    "\n",
    "#### PART B) RESULTS OF THE LL VALIDATED VCFS\n",
    "1- Reading ll validated vcfs.\n",
    "<br> 2- Creating a set for each dataframe with using \"CHROM\",\"POS\",\"REF\",\"ALT_1\" columns.\n",
    "\n",
    "#### PART C) SET OPERATIONS\n",
    "1- Creating a set operations funtions. \n",
    "<br>2- Creating a intersection array from each sets. \n",
    "<br>3- Show Intersection Results in heatmap\n",
    "\n",
    "#### PART D) COMPARISON WITH UNION VALIDATED VCF'S\n",
    "1- Create an union of all ll validated vfc sets for each data.\n",
    "<br>2- Show results in heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6a492",
   "metadata": {},
   "source": [
    "## ABOUT DATA\n",
    "50-51 : LL DATA\n",
    "<br>44-45 : NC DATA\n",
    "<br>79-80 : FD1 DATA\n",
    "<br>74-83 : Il_1 DATA\n",
    "<br>18-19 : EA DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c206e53",
   "metadata": {},
   "source": [
    "## IMPORT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "21811dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import allel\n",
    "import numpy as np # pip3 install matplotlib\n",
    "import matplotlib.pyplot as plt # pip3 install matplotlib\n",
    "\n",
    "import random\n",
    "import community # can be installed with: \"pip install python-louvain\"\n",
    "from numpy import linalg as LA\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import math\n",
    "import scipy.io as sio\n",
    "from skimage import data, segmentation, color\n",
    "from skimage.future import graph\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1151b",
   "metadata": {},
   "source": [
    "### Common Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bdc104a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting indels from VCF\n",
    "def remove_indels(t):\n",
    "    t = t[t[\"REF\"].apply(len) == 1 ]\n",
    "    t = t[t[\"ALT_1\"].apply(len) == 1]\n",
    "    return t \n",
    "\n",
    "#Normalization\n",
    "def normalization(df):\n",
    "    return MinMaxScaler().fit_transform(np.array(df['Results']).reshape(-1,1))\n",
    "\n",
    "#Apply Filters\n",
    "def filtersControl(key,df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    #Getting variantCaller Name\n",
    "    VCName = key.split('/')\n",
    "    \n",
    "    #Apply Filter Pass if not somaticSniper\n",
    "    df = df if VCName[len(VCName)-1] == 'somaticSniper' else df[df[\"FILTER_PASS\"]]\n",
    "    \n",
    "    #SOMATIC filter on strelka\n",
    "    df = df[df['SOMATIC'].astype(\"bool\")] if VCName[len(VCName)-1] == 'strelka' else df\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edc503",
   "metadata": {},
   "source": [
    "## PART A) RESULTS FROM THE PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1878aa7",
   "metadata": {},
   "source": [
    "### 1- Reading SNP vcf filenames generated as a result of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "27141192",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_filenames = {}\n",
    "for root, directories, filenames in os.walk('vcf/snp'):\n",
    "    for fn in filenames:\n",
    "        if fn == '.DS_Store':\n",
    "            continue;\n",
    "        else:\n",
    "            snp_filenames[root[8:]] = os.path.join(root,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "21585388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snp_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0daec4f",
   "metadata": {},
   "source": [
    "### 2- Creating a set for each dataframe with using \"CHROM\",\"POS\",\"REF\",\"ALT_1\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b33b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████                    | 15/28 [00:07<00:09,  1.37it/s]"
     ]
    }
   ],
   "source": [
    "sets_dict = {}\n",
    "\n",
    "filtering_df = pd.DataFrame(columns=[\"Key\", \"Total Length\", \"After FilterPass\", \"After RemoveIndels\"])\n",
    "\n",
    "for key in tqdm(snp_filenames):\n",
    "    \n",
    "    #Creating a dataframe for each vcf.\n",
    "    temp = allel.vcf_to_dataframe(snp_filenames[key],fields='variants/*')\n",
    "    #print(key)\n",
    "    #display(temp.head())\n",
    "    #Filtering - STARTS\n",
    "    total_len = len(temp)\n",
    "    temp = filtersControl(key,temp) #FilterPass control for somaticSniper\n",
    "    after_Filterpass_len = len(temp)\n",
    "    temp = remove_indels(temp) \n",
    "    without_indels_len = len(temp)\n",
    "    #temp = temp[temp[\"CHROM\"].apply(lambda x: \"_\" in x)]\n",
    "    #Filtering - ENDS\n",
    "    \n",
    "    #For printing changes in the vcf lengths\n",
    "    filtering_df.loc[len(filtering_df.index)] = [key , total_len, after_Filterpass_len,without_indels_len]\n",
    "    \n",
    "    #Creating a dictionary for each set\n",
    "    if len(temp) != 0:\n",
    "        sets_dict[key]= set(temp[[\"CHROM\",\"POS\",\"REF\",\"ALT_1\"]].apply(lambda row: \"_\".join(map(str, row)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "#len(sets_dict)\n",
    "#len(sets_dict['50_51/bowtie/octopus'])\n",
    "ax = filtering_df.plot('Key',kind='bar',title=\"The lenghts after 'FilterPass' and 'Removing Indels'\")\n",
    "for p in ax.patches:\n",
    "    if p.get_height() == 0:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83b7e2",
   "metadata": {},
   "source": [
    "## PART B) RESULTS OF THE LL VALIDATED VCFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b60943",
   "metadata": {},
   "source": [
    "### 1- Reading ll validated vcfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e969289",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_filenames = {}\n",
    "for root, directories, filenames in os.walk('vcf/validated_vcfs'):\n",
    "    for fn in filenames:\n",
    "        if fn == '.DS_Store':\n",
    "            continue;\n",
    "        else:\n",
    "            ll_filenames[root[19:]] = os.path.join(root,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62511112",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ll_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5291c20",
   "metadata": {},
   "source": [
    "### 2- Creating a set for each dataframe with using \"CHROM\",\"POS\",\"REF\",\"ALT_1\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_sets_dict = {}\n",
    "\n",
    "ll_filtering_df = pd.DataFrame(columns=[\"Key\", \"Total Length\", \"After FilterPass\", \"After RemoveIndels\"])\n",
    "\n",
    "for key in tqdm(ll_filenames):\n",
    "    #Creating a dataframe for each vcf.\n",
    "    temp = allel.vcf_to_dataframe(ll_filenames[key],fields='variants/*')\n",
    "    \n",
    "    #Filtering - STARTS\n",
    "    total_length = len(temp)\n",
    "    temp = filtersControl(key,temp) #FilterPass control for somaticSniper\n",
    "    after_filterpass = len(temp)\n",
    "    temp = remove_indels(temp)\n",
    "    after_remove_indels = len(temp)\n",
    "    #Filtering - ENDS\n",
    "    \n",
    "    #For printing changes in the vcf lengths\n",
    "    ll_filtering_df.loc[len(ll_filtering_df.index)] = [key , total_length, after_filterpass,after_remove_indels]\n",
    "\n",
    "    #Creating a set for each dataframe\n",
    "    if len(temp) != 0 :\n",
    "        ll_sets_dict[key]= set(temp[[\"CHROM\",\"POS\",\"REF\",\"ALT_1\"]].apply(lambda row: \"_\".join(map(str, row)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ll_filtering_df.plot('Key',kind='bar',title=\"The ll lenghts after 'FilterPass' and 'Removing Indels'\",figsize=(50,20))\n",
    "for p in ax.patches:\n",
    "    if p.get_height() == 0:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b7d0f",
   "metadata": {},
   "source": [
    "## PART C) SET OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca8698",
   "metadata": {},
   "source": [
    "\n",
    "### 1- Creating a set operations funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5419e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The intersection of two sets is the set of all the common elements of both the sets. \n",
    "def intersec(s1,s2):\n",
    "    return len(s1.intersection(s2))\n",
    "\n",
    "#The union of two sets is the set of all the elements of both the sets without duplicates.\n",
    "def uni(s1,s2):\n",
    "    return len(s1.union(s2))\n",
    "\n",
    "#The difference between two sets is the set of all the elements in first set that are not present in the second set. \n",
    "def dif(s1,s2):\n",
    "    return len(s1.difference(s2))\n",
    "\n",
    "#Jaccard Similarity function for two sets\n",
    "def js_set(s1, s2):\n",
    "    intersection = len(list(set(s1).intersection(s2)))\n",
    "    union = uni(s1,s2)\n",
    "    return np.float64(round((float(intersection) / union) * 100) / 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592a88e",
   "metadata": {},
   "source": [
    "### 2- Creating a intersection array from each sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca81a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a dataframe\n",
    "df_js = pd.DataFrame(columns=[\"LL_VCF\", \"Result_VCF\", \"Results\"])\n",
    "\n",
    "#Initializing a count for the rows\n",
    "count = 0\n",
    "\n",
    "#Computing each intersection of ll and the pipeline vcfs by looking at their key values. \n",
    "for key1 in tqdm(ll_sets_dict): \n",
    "    for key2 in sets_dict:\n",
    "        d_temp = pd.DataFrame([[key1, key2, js_set(ll_sets_dict[key1],sets_dict[key2])]],\n",
    "                   columns=['LL_VCF', 'Result_VCF',\"Results\"])\n",
    "        df_js = pd.concat((df_js,d_temp), ignore_index = True)\n",
    "    \n",
    "#Normalizing the results values with MinMaxScaler [0,1]\n",
    "#df_t['Results'] = MinMaxScaler().fit_transform(np.array(df_t['Results']).reshape(-1,1))\n",
    "\n",
    "#df_t\n",
    "df_js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4abb3",
   "metadata": {},
   "source": [
    "### 3- Show Intersection Results in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef14da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (72,28))\n",
    "df_h = df_js.pivot(\"Result_VCF\", \"LL_VCF\", \"Results\").fillna(0).apply(pd.to_numeric)\n",
    "ax = sns.heatmap(df_h, annot=False, cmap=\"YlGnBu\", linewidths=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e617d",
   "metadata": {},
   "source": [
    "### PART D) COMPARISON WITH UNION VALIDATED VCF'S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199ba51",
   "metadata": {},
   "source": [
    "#### 1- Create an union of all ll validated vfc sets for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_v_vcf = {}\n",
    "for key, value in ll_sets_dict.items():\n",
    "    union_v_vcf[key[:len(key.split('/')[0])]] = union_v_vcf.get(key[:len(key.split('/')[0])], set()) | value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498982f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing a dataframe\n",
    "df_union = pd.DataFrame(columns=[\"V_VCF\", \"Result_VCF\", \"Results\", 'length'])\n",
    "\n",
    "#Initializing a count for the rows\n",
    "count = 0\n",
    "\n",
    "#Computing each intersection of ll and the pipeline vcfs by looking at their key values. \n",
    "for key1 in sets_dict:\n",
    "    df_union[count] = [((key1.split('/')[0]).split('(')[0]).upper(), key1, js_set(union_v_vcf[((key1.split('/')[0]).split('(')[0]).upper()],sets_dict[key1]),len(union_v_vcf[((key1.split('/')[0]).split('(')[0]).upper()]) ]\n",
    "    count += 1\n",
    "\n",
    "#Taking transpose of the df and, getting rid of the first three rows then renaming axis again.\n",
    "df_union = df_union.transpose().iloc[4: , :].set_axis(['V_VCF', 'Result_VCF', 'Results', 'length'], axis=1, inplace=False)\n",
    "\n",
    "#df_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07eec6",
   "metadata": {},
   "source": [
    "#### 2- Show results in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union_h = df_union.pivot(\"Result_VCF\", \"V_VCF\", \"Results\").fillna(0).apply(pd.to_numeric)\n",
    "ax = sns.heatmap(df_union_h, annot=False,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fc7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f50181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbf093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92fc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265581ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1117eebb",
   "metadata": {},
   "source": [
    "Gül Eda Aydemir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63762d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
